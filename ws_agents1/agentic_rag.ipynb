{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir(\"/home/kantundpeterpan/projects/zoomcamp/zcllm/ws_agents1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, requests, dotenv\n",
    "from openai import OpenAI\n",
    "try:\n",
    "    if dotenv.load_dotenv(): \n",
    "        OPENROUTER_API_KEY = os.environ['OPENROUTER_API_KEY']\n",
    "except KeyError:\n",
    "    raise ValueError(\"set api key\")\n",
    "# You can use any model that supports tool calling\n",
    "MODEL = \"google/gemini-2.5-flash\"\n",
    "client = OpenAI(\n",
    "  base_url=\"https://openrouter.ai/api/v1\",\n",
    "  api_key=OPENROUTER_API_KEY,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests \n",
    "\n",
    "docs_url = 'https://github.com/alexeygrigorev/llm-rag-workshop/raw/main/notebooks/documents.json'\n",
    "docs_response = requests.get(docs_url)\n",
    "documents_raw = docs_response.json()\n",
    "\n",
    "documents = []\n",
    "\n",
    "for course in documents_raw:\n",
    "    course_name = course['course']\n",
    "\n",
    "    for doc in course['documents']:\n",
    "        doc['course'] = course_name\n",
    "        documents.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<minsearch.append.AppendableIndex at 0x7f691d5eecf0>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from minsearch import AppendableIndex\n",
    "\n",
    "index = AppendableIndex(\n",
    "    text_fields=[\"question\", \"text\", \"section\"],\n",
    "    keyword_fields=[\"course\"]\n",
    ")\n",
    "\n",
    "index.fit(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant. Answer the QUESTION based on the CONTEXT from the FAQ database.\n",
    "Use only the facts from the CONTEXT when answering the QUESTION.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT>\n",
    "{context}\n",
    "</CONTEXT>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_prompt(query, search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "    \n",
    "    prompt = prompt_template.format(question=query, context=context).strip()\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def llm(prompt):\n",
    "    response = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[{\"role\": \"user\", \"content\": prompt}]\n",
    "    )\n",
    "    return response.choices[0].message.content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rag(query):\n",
    "    search_results = search(query)\n",
    "    prompt = build_prompt(query, search_results)\n",
    "    answer = llm(prompt)\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Yes, you can still join the course even after the start date. You are eligible to submit homeworks even if you haven't registered. However, be aware of the deadlines for final projects to avoid last-minute rush.\n"
     ]
    }
   ],
   "source": [
    "print(rag('Can i join after the course started ?'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic RAG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "At the beginning the context is EMPTY.\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "If CONTEXT is EMPTY, you can use our FAQ database IF you deem the question related to one of the courses\n",
    "you are responsible for: Machine Learning, MLOps, Data Engineering and LLM Applications.\n",
    "In this case, use the following output template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\"\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'Can I still join the course ?'\n",
    "context = \"EMPTY\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = prompt_template.format(\n",
    "    question = question,\n",
    "    context = context\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer = json.loads(answer_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'SEARCH'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "answer['action']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_context(search_results):\n",
    "    context = \"\"\n",
    "\n",
    "    for doc in search_results:\n",
    "        context = context + f\"section: {doc['section']}\\nquestion: {doc['question']}\\nanswer: {doc['text']}\\n\\n\"\n",
    "\n",
    "    return context.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "At the beginning the context is EMPTY.\n",
      "\n",
      "<QUESTION>\n",
      "Can I still join the course ?\n",
      "</QUESTION>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - I have registered for the Data Engineering Bootcamp. When can I expect to receive the confirmation email?\n",
      "answer: You don't need it. You're accepted. You can also just start learning and submitting homework without registering. It is not checked against any registered list. Registration is just to gauge interest before the start date.\n",
      "</CONTEXT>\n",
      "\n",
      "If CONTEXT is EMPTY, you can use our FAQ database IF you deem the question related to one of the courses\n",
      "you are responsible for: Machine Learning, MLOps, Data Engineering and LLM Applications.\n",
      "In this case, use the following output template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\"\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "search_results = search(question)\n",
    "context = build_context(search_results)\n",
    "prompt = prompt_template.format(question=question, context=context)\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "answer_json = llm(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"Yes, you can still join the course even after the official start date and submit homeworks. However, be mindful of deadlines for the final projects.\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(answer_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Puttin agentic RAG together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_rag_v1(question):\n",
    "    context = \"EMPTY\"\n",
    "    prompt = prompt_template.format(question=question, context=context)\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(answer)\n",
    "\n",
    "    if answer['action'] == 'SEARCH':\n",
    "        print('need to perform search...')\n",
    "        search_results = search(question)\n",
    "        context = build_context(search_results)\n",
    "        \n",
    "        prompt = prompt_template.format(question=question, context=context)\n",
    "        answer_json = llm(prompt)\n",
    "        answer = json.loads(answer_json)\n",
    "        print(answer)\n",
    "\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'SEARCH', 'reasoning': 'The user is asking a general question about joining a course. This is a common question that might be covered in an FAQ for any of the courses I support (Machine Learning, MLOps, Data Engineering, LLM Applications).'}\n",
      "need to perform search...\n",
      "{'action': 'ANSWER', 'answer': 'To join the course, you should register using the provided link before it starts. The course begins on January 15th, 2024, at 17:00 with the first \"Office Hours\" live session. You can also subscribe to the course\\'s public Google Calendar, join the course Telegram channel for announcements, and register in DataTalks.Club\\'s Slack to join the channel.', 'source': 'CONTEXT'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': 'To join the course, you should register using the provided link before it starts. The course begins on January 15th, 2024, at 17:00 with the first \"Office Hours\" live session. You can also subscribe to the course\\'s public Google Calendar, join the course Telegram channel for announcements, and register in DataTalks.Club\\'s Slack to join the channel.',\n",
       " 'source': 'CONTEXT'}"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how do I join the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'action': 'SEARCH', 'reasoning': \"The question is about patching KDE under FreeBSD, which is an operating system and desktop environment topic. This is not directly related to Machine Learning, MLOps, Data Engineering, or LLM Applications, and therefore likely not in our course FAQ database. A search action is chosen to indicate that it's outside the scope of direct course material but a search would be the next logical step if it were related.\"}\n",
      "need to perform search...\n",
      "{'action': 'ANSWER', 'answer': \"Patching KDE under FreeBSD typically involves using the FreeBSD Ports or Packages system. The most common and recommended way to update or patch software, including desktop environments like KDE, on FreeBSD is through the `pkg` command for binary packages or by updating and re-installing from the Ports tree. \\n\\n1.  **Using `pkg` (for binary packages):**\\n    If you installed KDE using `pkg install kde5` (or similar), you can update it by running `pkg upgrade`. This command will fetch and install the latest versions of all installed packages, including KDE components, from the official FreeBSD package repositories.\\n\\n2.  **Using Ports (for source-based installation):**\\n    If you built KDE from the Ports tree (i.e., you navigated to `/usr/ports/x11/kde5` and ran `make install` or `make reinstall`), you'll need to update your Ports tree first, and then rebuild/reinstall KDE. \\n    *   Update your Ports tree: `portsnap fetch update` (or `git pull` if you're using a Git-based ports tree).\\n    *   Navigate to the KDE port directory: `cd /usr/ports/x11/kde5` (or the specific KDE component you want to update, e.g., `x11/plasma5-plasma` for Plasma desktop itself).\\n    *   Clean and rebuild/reinstall: `make clean install` or `make clean reinstall`. `make reinstall` is generally safer as it attempts to preserve configuration where possible, but `make install` will also work if the application allows it. You might need to add `BATCH=yes` to the `make` command if you don't want to be prompted for configuration options for all dependencies.\\n\\n    *Important Note for Ports:* Updating extensively via ports can be time-consuming and sometimes complex due to dependencies. Using `portmaster` or `portupgrade` utilities can significantly simplify the process of managing and updating ports, as they can automate dependency handling and rebuilding.\\n\\n    *   Install `portmaster`: `pkg install portmaster`\\n    *   Update all installed ports: `portmaster -Da` (This will update everything, including KDE).\\n\\nBefore undertaking any major update, especially with ports, it's always a good idea to back up important configuration files and check the FreeBSD forums, mailing lists, or release notes for any known issues with the specific KDE version you're upgrading to.\\n\\nIn summary, for most users: `pkg upgrade` is the simplest way to get security patches and updates. For those who compiled from source: update your ports tree and use `portmaster` or `make clean reinstall` on the relevant KDE ports.\", 'source': 'OWN_KNOWLEDGE'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'action': 'ANSWER',\n",
       " 'answer': \"Patching KDE under FreeBSD typically involves using the FreeBSD Ports or Packages system. The most common and recommended way to update or patch software, including desktop environments like KDE, on FreeBSD is through the `pkg` command for binary packages or by updating and re-installing from the Ports tree. \\n\\n1.  **Using `pkg` (for binary packages):**\\n    If you installed KDE using `pkg install kde5` (or similar), you can update it by running `pkg upgrade`. This command will fetch and install the latest versions of all installed packages, including KDE components, from the official FreeBSD package repositories.\\n\\n2.  **Using Ports (for source-based installation):**\\n    If you built KDE from the Ports tree (i.e., you navigated to `/usr/ports/x11/kde5` and ran `make install` or `make reinstall`), you'll need to update your Ports tree first, and then rebuild/reinstall KDE. \\n    *   Update your Ports tree: `portsnap fetch update` (or `git pull` if you're using a Git-based ports tree).\\n    *   Navigate to the KDE port directory: `cd /usr/ports/x11/kde5` (or the specific KDE component you want to update, e.g., `x11/plasma5-plasma` for Plasma desktop itself).\\n    *   Clean and rebuild/reinstall: `make clean install` or `make clean reinstall`. `make reinstall` is generally safer as it attempts to preserve configuration where possible, but `make install` will also work if the application allows it. You might need to add `BATCH=yes` to the `make` command if you don't want to be prompted for configuration options for all dependencies.\\n\\n    *Important Note for Ports:* Updating extensively via ports can be time-consuming and sometimes complex due to dependencies. Using `portmaster` or `portupgrade` utilities can significantly simplify the process of managing and updating ports, as they can automate dependency handling and rebuilding.\\n\\n    *   Install `portmaster`: `pkg install portmaster`\\n    *   Update all installed ports: `portmaster -Da` (This will update everything, including KDE).\\n\\nBefore undertaking any major update, especially with ports, it's always a good idea to back up important configuration files and check the FreeBSD forums, mailing lists, or release notes for any known issues with the specific KDE version you're upgrading to.\\n\\nIn summary, for most users: `pkg upgrade` is the simplest way to get security patches and updates. For those who compiled from source: update your ports tree and use `portmaster` or `make clean reinstall` on the relevant KDE ports.\",\n",
       " 'source': 'OWN_KNOWLEDGE'}"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "agentic_rag_v1('how patch KDE under FreeBSD?')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agentic Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agentic Search is an extension of Agentic RAG by leveraging an LLM Agent to formulate search queries based on the initial question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = 'how do I do well in module 1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "\n",
    "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
    "\n",
    "The CONTEXT is build with the documents from our FAQ database.\n",
    "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
    "from FAQ to and add them to the context.\n",
    "PREVIOUS_ACTIONS contains the actions you already performed.\n",
    "\n",
    "At the beginning the CONTEXT is empty.\n",
    "\n",
    "You can perform the following actions:\n",
    "\n",
    "- Search in the FAQ database to get more data for the CONTEXT\n",
    "- Answer the question using the CONTEXT\n",
    "- Answer the question using your own knowledge\n",
    "\n",
    "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
    "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
    "\n",
    "Don't use search queries used at the previous iterations.\n",
    "\n",
    "Don't repeat previously performed actions.\n",
    "\n",
    "Don't perform more than {max_iterations} iterations for a given student question.\n",
    "The current iteration number: {iteration_number}. If we exceed the allowed number \n",
    "of iterations, give the best possible answer with the provided information.\n",
    "\n",
    "Output templates:\n",
    "\n",
    "If you want to perform search, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"SEARCH\",\n",
    "\"reasoning\": \"<add your reasoning here>\",\n",
    "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
    "}}\n",
    "\n",
    "If you can answer the QUESTION using CONTEXT, use this template:\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER_CONTEXT\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"CONTEXT\"\n",
    "}}\n",
    "\n",
    "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
    "\n",
    "{{\n",
    "\"action\": \"ANSWER\",\n",
    "\"answer\": \"<your answer>\",\n",
    "\"source\": \"OWN_KNOWLEDGE\"\n",
    "}}\n",
    "\n",
    "<QUESTION>\n",
    "{question}\n",
    "</QUESTION>\n",
    "\n",
    "<SEARCH_QUERIES>\n",
    "{search_queries}\n",
    "</SEARCH_QUERIES>\n",
    "\n",
    "<CONTEXT> \n",
    "{context}\n",
    "</CONTEXT>\n",
    "\n",
    "<PREVIOUS_ACTIONS>\n",
    "{previous_actions}\n",
    "</PREVIOUS_ACTIONS>\n",
    "\"\"\".strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "question = \"how do I join the course?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=1\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The user is asking how to join the course. I need to search the FAQ for information on course enrollment or joining procedures.\",\n",
      "  \"keywords\": [\n",
      "    \"join course\",\n",
      "    \"enrollment process\",\n",
      "    \"how to register for the course\"\n",
      "  ]\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "previous_actions.append(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'action': 'SEARCH',\n",
       "  'reasoning': 'The user is asking how to join the course. I need to search the FAQ for information on course enrollment or joining procedures.',\n",
       "  'keywords': ['join course',\n",
       "   'enrollment process',\n",
       "   'how to register for the course']}]"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "previous_actions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = answer['keywords']\n",
    "search_queries.extend(keywords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in keywords:\n",
    "    res = search(k)\n",
    "    search_results.extend(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dedup(seq):\n",
    "    seen = set()\n",
    "    result = []\n",
    "    for el in seq:\n",
    "        _id = el['_id']\n",
    "        if _id in seen:\n",
    "            continue\n",
    "        seen.add(_id)\n",
    "        result.append(el)\n",
    "    return result\n",
    "\n",
    "search_results = dedup(search_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 2. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I join the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "join course\n",
      "enrollment process\n",
      "how to register for the course\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - Can I still join the course after the start date?\n",
      "answer: Yes, even if you don't register, you're still eligible to submit the homeworks.\n",
      "Be aware, however, that there will be deadlines for turning in the final projects. So don't leave everything for the last minute.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: lsRuntimeError: Java gateway process exited before sending its port number\n",
      "answer: After installing all including pyspark (and it is successfully imported), but then running this script on the jupyter notebook\n",
      "import pyspark\n",
      "from pyspark.sql import SparkSession\n",
      "spark = SparkSession.builder \\\n",
      ".master(\"local[*]\") \\\n",
      ".appName('test') \\\n",
      ".getOrCreate()\n",
      "df = spark.read \\\n",
      ".option(\"header\", \"true\") \\\n",
      ".csv('taxi+_zone_lookup.csv')\n",
      "df.show()\n",
      "it gives the error:\n",
      "RuntimeError: Java gateway process exited before sending its port number\n",
      "✅The solution (for me) was:\n",
      "pip install findspark on the command line and then\n",
      "Add\n",
      "import findspark\n",
      "findspark.init()\n",
      "to the top of the script.\n",
      "Another possible solution is:\n",
      "Check that pyspark is pointing to the correct location.\n",
      "Run pyspark.__file__. It should be list /home/<your user name>/spark/spark-3.0.3-bin-hadoop3.2/python/pyspark/__init__.py if you followed the videos.\n",
      "If it is pointing to your python site-packages remove the pyspark directory there and check that you have added the correct exports to you .bashrc file and that there are not any other exports which might supersede the ones provided in the course content.\n",
      "To add to the solution above, if the errors persist in regards to setting the correct path for spark,  an alternative solution for permanent path setting solve the error is  to set environment variables on system and user environment variables following this tutorial: Install Apache PySpark on Windows PC | Apache Spark Installation Guide\n",
      "Once everything is installed, skip to 7:14 to set up environment variables. This allows for the environment variables to be set permanently.\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: Process to download the VSC using Pandas is killed right away\n",
      "answer: pd.read_csv\n",
      "df_iter = pd.read_csv(dataset_url, iterator=True, chunksize=100000)\n",
      "The data needs to be appended to the parquet file using the fastparquet engine\n",
      "df.to_parquet(path, compression=\"gzip\", engine='fastparquet', append=True)\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Docker-Compose - Errors pertaining to docker-compose.yml and pgadmin setup\n",
      "answer: For everyone who's having problem with Docker compose, getting the data in postgres and similar issues, please take care of the following:\n",
      "create a new volume on docker (either using the command line or docker desktop app)\n",
      "make the following changes to your docker-compose.yml file (see attachment)\n",
      "set low_memory=false when importing the csv file (df = pd.read_csv('yellow_tripdata_2021-01.csv', nrows=1000, low_memory=False))\n",
      "use the below function (in the upload-data.ipynb) for better tracking of your ingestion process (see attachment)\n",
      "Order of execution:\n",
      "(1) open terminal in 2_docker_sql folder and run docker compose up\n",
      "(2) ensure no other containers are running except the one you just executed (pgadmin and pgdatabase)\n",
      "(3) open jupyter notebook and begin the data ingestion\n",
      "(4) open pgadmin and set up a server (make sure you use the same configurations as your docker-compose.yml file like the same name (pgdatabase), port, databasename (ny_taxi) etc.\n",
      "\n",
      "section: Module 2: Workflow Orchestration\n",
      "question: GCP - 2.2.7d Part 2 - Getting error when you run terraform apply\n",
      "answer: If you get the following error\n",
      "You have to edit variables.tf on the gcp folder, set your project-id and region and zones properly. Then, run terraform apply again.\n",
      "You can find correct regions/zones here: https://cloud.google.com/compute/docs/regions-zones\n",
      "Deploying MAGE to GCP  with Terraform via the VM (2.2.7)\n",
      "FYI - It can take up to 20 minutes to deploy the MAGE Terraform files if you are using a GCP Virtual Machine. It is normal, so don’t interrupt the process or think it’s taking too long. If you have, make sure you run a terraform destroy before trying again as you will have likely partially created resources which will cause errors next time you run `terraform apply`.\n",
      "`terraform destroy` may not completely delete partial resources - go to Google Cloud Console and use the search bar at the top to search for the ‘app.name’ you declared in your variables.tf file; this will list all resources with that name - make sure you delete them all before running `terraform apply` again.\n",
      "Why are my GCP free credits going so fast? MAGE .tf files - Terraform Destroy not destroying all Resources\n",
      "I checked my GCP billing last night & the MAGE Terraform IaC didn't destroy a GCP Resource called Filestore as ‘mage-data-prep- it has been costing £5.01 of my free credits each day  I now have £151 left - Alexey has assured me that This amount WILL BE SUFFICIENT funds to finish the course. Note to anyone who had issues deploying the MAGE terraform code: check your billing account to see what you're being charged for (main menu - billing) (even if it's your free credits) and run a search for 'mage-data-prep' in the top bar just to be sure that your resources have been destroyed - if any come up delete them.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The user is asking how to join the course. I need to search the FAQ for information on course enrollment or joining procedures.\", \"keywords\": [\"join course\", \"enrollment process\", \"how to register for the course\"]}\n",
      "</PREVIOUS_ACTIONS>\n"
     ]
    }
   ],
   "source": [
    "context = build_context(search_results)\n",
    "\n",
    "prompt = prompt_template.format(\n",
    "    question=question,\n",
    "    context=context,\n",
    "    search_queries=\"\\n\".join(search_queries),\n",
    "    previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "    max_iterations=3,\n",
    "    iteration_number=2\n",
    ")\n",
    "print(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To join the course, you should register using the provided link. You can also subscribe to the course public Google Calendar (from desktop only) and join the course Telegram channel for announcements. Additionally, register in DataTalks.Club's Slack and join the relevant channel. Even if you don't register, you can still submit homeworks, but be aware of deadlines for final projects. If you want a certificate, you must finish the course with a \\\"live\\\" cohort, as certificates are not awarded for self-paced mode.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer_json = llm(prompt)\n",
    "answer = json.loads(answer_json)\n",
    "print(json.dumps(answer, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The user is asking for advice on how to be successful in Module 1. I need to search the FAQ database for information specific to Module 1 and general success tips for modules.\",\n",
      "  \"keywords\": [\n",
      "    \"Module 1 success tips\",\n",
      "    \"how to succeed in Module 1\",\n",
      "    \"Module 1 requirements\",\n",
      "    \"Module 1 study guide\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "what do I need to do to be successful at module 1?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "Module 1 study guide\n",
      "Module 1 success tips\n",
      "Module 1 requirements\n",
      "how to succeed in Module 1\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: Module 5: pyspark\n",
      "question: Module Not Found Error in Jupyter Notebook .\n",
      "answer: Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\n",
      "The solution which worked for me(use following in jupyter notebook) :\n",
      "!pip install findspark\n",
      "import findspark\n",
      "findspark.init()\n",
      "Thereafter , import pyspark and create spark contex<<t as usual\n",
      "None of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\n",
      "Filter based on conditions based on multiple columns\n",
      "from pyspark.sql.functions import col\n",
      "new_final.filter((new_final.a_zone==\"Murray Hill\") & (new_final.b_zone==\"Midwood\")).show()\n",
      "Krishna Anand\n",
      "\n",
      "section: Module 5: pyspark\n",
      "question: Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\n",
      "answer: You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\n",
      "` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\n",
      "export PYTHONPATH=”${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}”\n",
      "Make sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\n",
      "For instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\n",
      "Then the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\"` appropriately.\n",
      "Additionally, you can check for the version of ‘py4j’ of the spark you’re using from here and update as mentioned above.\n",
      "~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\n",
      "\n",
      "section: Module 4: analytics engineering with dbt\n",
      "question: DBT - Error: No module named 'pytz' while setting up dbt with docker\n",
      "answer: Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\n",
      "Solution:\n",
      "Add `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Postgres - ModuleNotFoundError: No module named 'psycopg2'\n",
      "answer: Issue:\n",
      "e…\n",
      "Solution:\n",
      "pip install psycopg2-binary\n",
      "If you already have it, you might need to update it:\n",
      "pip install psycopg2-binary --upgrade\n",
      "Other methods, if the above fails:\n",
      "if you are getting the “ ModuleNotFoundError: No module named 'psycopg2' “ error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\n",
      "First uninstall the psycopg package\n",
      "Then update conda or pip\n",
      "Then install psycopg again using pip.\n",
      "if you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLALchemy - TypeError 'module' object is not callable\n",
      "answer: create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \"TypeError: 'module' object is not callable\"\n",
      "Solution:\n",
      "conn_string = \"postgresql+psycopg://root:root@localhost:5432/ny_taxi\"\n",
      "engine = create_engine(conn_string)\n",
      "\n",
      "section: Module 6: streaming with kafka\n",
      "question: Module “kafka” not found when trying to run producer.py\n",
      "answer: Solution from Alexey: create a virtual environment and run requirements.txt and the python files in that environment.\n",
      "To create a virtual env and install packages (run only once)\n",
      "python -m venv env\n",
      "source env/bin/activate\n",
      "pip install -r ../requirements.txt\n",
      "To activate it (you'll need to run it every time you need the virtual env):\n",
      "source env/bin/activate\n",
      "To deactivate it:\n",
      "deactivate\n",
      "This works on MacOS, Linux and Windows - but for Windows the path is slightly different (it's env/Scripts/activate)\n",
      "Also the virtual environment should be created only to run the python file. Docker images should first all be up and running.\n",
      "\n",
      "section: Module 1: Docker and Terraform\n",
      "question: Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'.\n",
      "answer: Error raised during the jupyter notebook’s cell execution:\n",
      "engine = create_engine('postgresql://root:root@localhost:5432/ny_taxi').\n",
      "Solution: Need to install Python module “psycopg2”. Can be installed by Conda or pip.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The user is asking for advice on how to be successful in Module 1. I need to search the FAQ database for information specific to Module 1 and general success tips for modules.\", \"keywords\": [\"Module 1 success tips\", \"how to succeed in Module 1\", \"Module 1 requirements\", \"Module 1 study guide\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To be successful in Module 1, which focuses on Docker and Terraform, you should be aware of and prepared to handle common technical issues. Based on the provided context, some frequently encountered problems and their solutions include:\\n\\n*   **Postgres - ModuleNotFoundError: No module named 'psycopg2'**: This error often occurs when the `psycopg2` Python module is not installed or needs to be updated. The solution is to run `pip install psycopg2-binary` or `pip install psycopg2-binary --upgrade`. If issues persist, try uninstalling, updating pip/conda, and then reinstalling. For Mac users, installing PostgreSQL via `brew install postgresql` might be necessary if `pg_config` is not found.\\n*   **Python - SQLAlchemy - TypeError 'module' object is not callable**: This error, typically encountered when setting up the database engine, can be resolved by correctly formatting the connection string. For example, use `conn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"` and then `engine = create_engine(conn_string)`.\\n*   **Python - SQLAlchemy - ModuleNotFoundError: No module named 'psycopg2'**: Similar to the Postgres specific `ModuleNotFoundError`, this indicates the `psycopg2` Python module is missing. The solution is to install it using Conda or pip.\\n\\nWhile the context mainly provides solutions to common technical roadblocks in Module 1, successfully navigating these issues is a key part of progressing through the module. Beyond troubleshooting, general success in any module typically involves understanding the core concepts, actively participating, and practicing the skills taught.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "question = \"what do I need to do to be successful at module 1?\"\n",
    "\n",
    "search_queries = []\n",
    "search_results = []\n",
    "previous_actions = []\n",
    "\n",
    "\n",
    "iteration = 0\n",
    "\n",
    "while True:\n",
    "    print(f'ITERATION #{iteration}...')\n",
    "\n",
    "    context = build_context(search_results)\n",
    "    prompt = prompt_template.format(\n",
    "        question=question,\n",
    "        context=context,\n",
    "        search_queries=\"\\n\".join(search_queries),\n",
    "        previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "        max_iterations=3,\n",
    "        iteration_number=iteration\n",
    "    )\n",
    "\n",
    "    print(prompt)\n",
    "\n",
    "    answer_json = llm(prompt)\n",
    "    answer = json.loads(answer_json)\n",
    "    print(json.dumps(answer, indent=2))\n",
    "\n",
    "    previous_actions.append(answer)\n",
    "\n",
    "    action = answer['action']\n",
    "    if action != 'SEARCH':\n",
    "        break\n",
    "\n",
    "    keywords = answer['keywords']\n",
    "    search_queries = list(set(search_queries) | set(keywords))\n",
    "    \n",
    "    for k in keywords:\n",
    "        res = search(k)\n",
    "        search_results.extend(res)\n",
    "\n",
    "    search_results = dedup(search_results)\n",
    "    \n",
    "    iteration = iteration + 1\n",
    "    if iteration >= 4:\n",
    "        break\n",
    "\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "def agentic_search(question):\n",
    "    # Initialize lists to store search queries, results, and previous agent actions\n",
    "    search_queries = []\n",
    "    search_results = []\n",
    "    previous_actions = []\n",
    "\n",
    "    # Initialize iteration counter\n",
    "    iteration = 0\n",
    "    \n",
    "    # Start the main loop for iterative search\n",
    "    while True:\n",
    "        # Print current iteration number for debugging/tracking\n",
    "        print(f'ITERATION #{iteration}...')\n",
    "    \n",
    "        # Build context from previous search results to inform the LLM\n",
    "        context = build_context(search_results)\n",
    "        # Format the prompt using a predefined template and current state\n",
    "        prompt = prompt_template.format(\n",
    "            question=question,\n",
    "            context=context,\n",
    "            search_queries=\"\\n\".join(search_queries),\n",
    "            previous_actions='\\n'.join([json.dumps(a) for a in previous_actions]),\n",
    "            max_iterations=3, # Note: This parameter is in the prompt, but the actual loop limit is 4.\n",
    "            iteration_number=iteration\n",
    "        )\n",
    "    \n",
    "        # Print the prompt sent to the LLM\n",
    "        print(prompt)\n",
    "    \n",
    "        # Call the LLM with the formatted prompt to get an action\n",
    "        answer_json = llm(prompt)\n",
    "        # Parse the JSON response from the LLM\n",
    "        answer = json.loads(answer_json)\n",
    "        # Print the LLM's response for debugging/tracking\n",
    "        print(json.dumps(answer, indent=2))\n",
    "\n",
    "        # Record the current action in previous_actions for future iterations\n",
    "        previous_actions.append(answer)\n",
    "    \n",
    "        # Get the action type from the LLM's response\n",
    "        action = answer['action']\n",
    "        # If the action is not 'SEARCH', break the loop (e.g., if it's 'FINISH' or 'ANSWER')\n",
    "        if action != 'SEARCH':\n",
    "            break\n",
    "    \n",
    "        # Extract keywords for searching from the LLM's response\n",
    "        keywords = answer['keywords']\n",
    "        # Update the list of all search queries, ensuring uniqueness\n",
    "        search_queries = list(set(search_queries) | set(keywords))\n",
    "\n",
    "        # Perform searches for each new keyword\n",
    "        for k in keywords:\n",
    "            res = search(k)\n",
    "            # Add new results to the overall search_results list\n",
    "            search_results.extend(res)\n",
    "    \n",
    "        # Deduplicate search results to avoid redundant information\n",
    "        search_results = dedup(search_results)\n",
    "        \n",
    "        # Increment the iteration counter\n",
    "        iteration = iteration + 1\n",
    "        # Set a hard limit on the number of iterations to prevent infinite loops\n",
    "        if iteration >= 4:\n",
    "            break\n",
    "    \n",
    "        # Print a newline for better readability between iterations\n",
    "        print()\n",
    "\n",
    "    # Return the final answer from the LLM\n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ITERATION #0...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 0. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"SEARCH\",\n",
      "  \"reasoning\": \"The user is asking about course preparation. I should search the FAQ for information related to course preparation, prerequisites, or introductory materials.\",\n",
      "  \"keywords\": [\n",
      "    \"course preparation\",\n",
      "    \"how to prepare for course\",\n",
      "    \"prerequisites for course\"\n",
      "  ]\n",
      "}\n",
      "\n",
      "ITERATION #1...\n",
      "You're a course teaching assistant.\n",
      "\n",
      "You're given a QUESTION from a course student and that you need to answer with your own knowledge and provided CONTEXT.\n",
      "\n",
      "The CONTEXT is build with the documents from our FAQ database.\n",
      "SEARCH_QUERIES contains the queries that were used to retrieve the documents\n",
      "from FAQ to and add them to the context.\n",
      "PREVIOUS_ACTIONS contains the actions you already performed.\n",
      "\n",
      "At the beginning the CONTEXT is empty.\n",
      "\n",
      "You can perform the following actions:\n",
      "\n",
      "- Search in the FAQ database to get more data for the CONTEXT\n",
      "- Answer the question using the CONTEXT\n",
      "- Answer the question using your own knowledge\n",
      "\n",
      "For the SEARCH action, build search requests based on the CONTEXT and the QUESTION.\n",
      "Carefully analyze the CONTEXT and generate the requests to deeply explore the topic. \n",
      "\n",
      "Don't use search queries used at the previous iterations.\n",
      "\n",
      "Don't repeat previously performed actions.\n",
      "\n",
      "Don't perform more than 3 iterations for a given student question.\n",
      "The current iteration number: 1. If we exceed the allowed number \n",
      "of iterations, give the best possible answer with the provided information.\n",
      "\n",
      "Output templates:\n",
      "\n",
      "If you want to perform search, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"SEARCH\",\n",
      "\"reasoning\": \"<add your reasoning here>\",\n",
      "\"keywords\": [\"search query 1\", \"search query 2\", ...]\n",
      "}\n",
      "\n",
      "If you can answer the QUESTION using CONTEXT, use this template:\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER_CONTEXT\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"CONTEXT\"\n",
      "}\n",
      "\n",
      "If the context doesn't contain the answer, use your own knowledge to answer the question\n",
      "\n",
      "{\n",
      "\"action\": \"ANSWER\",\n",
      "\"answer\": \"<your answer>\",\n",
      "\"source\": \"OWN_KNOWLEDGE\"\n",
      "}\n",
      "\n",
      "<QUESTION>\n",
      "how do I prepare for the course?\n",
      "</QUESTION>\n",
      "\n",
      "<SEARCH_QUERIES>\n",
      "course preparation\n",
      "how to prepare for course\n",
      "prerequisites for course\n",
      "</SEARCH_QUERIES>\n",
      "\n",
      "<CONTEXT> \n",
      "section: General course-related questions\n",
      "question: Course - When will the course start?\n",
      "answer: The purpose of this document is to capture frequently asked technical questions\n",
      "The exact day and hour of the course will be 15th Jan 2024 at 17h00. The course will start with the first  “Office Hours'' live.1\n",
      "Subscribe to course public Google Calendar (it works from Desktop only).\n",
      "Register before the course starts using this link.\n",
      "Join the course Telegram channel with announcements.\n",
      "Don’t forget to register in DataTalks.Club's Slack and join the channel.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - Can I follow the course after it finishes?\n",
      "answer: Yes, we will keep all the materials after the course finishes, so you can follow the course at your own pace after it finishes.\n",
      "You can also continue looking at the homeworks and continue preparing for the next cohort. I guess you can also start working on your final capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Certificate - Can I follow the course in a self-paced mode and get a certificate?\n",
      "answer: No, you can only get a certificate if you finish the course with a “live” cohort. We don't award certificates for the self-paced mode. The reason is you need to peer-review capstone(s) after submitting a project. You can only peer-review projects at the time the course is running.\n",
      "\n",
      "section: General course-related questions\n",
      "question: Is it possible to use tool “X” instead of the one tool you use in the course?\n",
      "answer: Yes, this applies if you want to use Airflow or Prefect instead of Mage, AWS or Snowflake instead of GCP products or Tableau instead of Metabase or Google data studio.\n",
      "The course covers 2 alternative data stacks, one using GCP and one using local installation of everything. You can use one of them or use your tool of choice.\n",
      "Should you consider it instead of the one tool you use? That we can’t support you if you choose to use a different stack, also you would need to explain the different choices of tool for the peer review of your capstone project.\n",
      "\n",
      "section: General course-related questions\n",
      "question: How do I use Git / GitHub for this course?\n",
      "answer: After you create a GitHub account, you should clone the course repo to your local machine using the process outlined in this video: Git for Everybody: How to Clone a Repository from GitHub\n",
      "Having this local repository on your computer will make it easy for you to access the instructors’ code and make pull requests (if you want to add your own notes or make changes to the course content).\n",
      "You will probably also create your own repositories that host your notes, versions of your file, to do this. Here is a great tutorial that shows you how to do this: https://www.atlassian.com/git/tutorials/setting-up-a-repository\n",
      "Remember to ignore large database, .csv, and .gz files, and other files that should not be saved to a repository. Use .gitignore for this: https://www.atlassian.com/git/tutorials/saving-changes/gitignore NEVER store passwords or keys in a git repo (even if that repo is set to private).\n",
      "This is also a great resource: https://dangitgit.com/\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - What are the prerequisites for this course?\n",
      "answer: GitHub - DataTalksClub data-engineering-zoomcamp#prerequisites\n",
      "\n",
      "section: General course-related questions\n",
      "question: Course - What can I do before the course starts?\n",
      "answer: You can start by installing and setting up all the dependencies and requirements:\n",
      "Google cloud account\n",
      "Google Cloud SDK\n",
      "Python 3 (installed with Anaconda)\n",
      "Terraform\n",
      "Git\n",
      "Look over the prerequisites and syllabus to see if you are comfortable with these subjects.\n",
      "</CONTEXT>\n",
      "\n",
      "<PREVIOUS_ACTIONS>\n",
      "{\"action\": \"SEARCH\", \"reasoning\": \"The user is asking about course preparation. I should search the FAQ for information related to course preparation, prerequisites, or introductory materials.\", \"keywords\": [\"course preparation\", \"how to prepare for course\", \"prerequisites for course\"]}\n",
      "</PREVIOUS_ACTIONS>\n",
      "{\n",
      "  \"action\": \"ANSWER_CONTEXT\",\n",
      "  \"answer\": \"To prepare for the course, you can start by installing and setting up the necessary dependencies and requirements, which include a Google Cloud account, Google Cloud SDK, Python 3 (installed with Anaconda), Terraform, and Git. It's also recommended to review the prerequisites and syllabus to ensure you are comfortable with the subjects covered.\\n\\nAdditionally, you should clone the course repository from GitHub to your local machine. For using Git and GitHub, resources like \\\"Git for Everybody: How to Clone a Repository from GitHub\\\" and a tutorial on setting up a repository (https://www.atlassian.com/git/tutorials/setting-up-a-repository) are suggested. Remember to use `.gitignore` for large files and never store passwords or keys in a Git repository.\",\n",
      "  \"source\": \"CONTEXT\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "answer = agentic_search('how do I prepare for the course?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import display_markdown, Markdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To prepare for the course, you can start by installing and setting up the necessary dependencies and requirements, which include a Google Cloud account, Google Cloud SDK, Python 3 (installed with Anaconda), Terraform, and Git. It's also recommended to review the prerequisites and syllabus to ensure you are comfortable with the subjects covered.\n",
       "\n",
       "Additionally, you should clone the course repository from GitHub to your local machine. For using Git and GitHub, resources like \"Git for Everybody: How to Clone a Repository from GitHub\" and a tutorial on setting up a repository (https://www.atlassian.com/git/tutorials/setting-up-a-repository) are suggested. Remember to use `.gitignore` for large files and never store passwords or keys in a Git repository."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(answer['answer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Brave Search API](https://brave.com/search/api/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function calling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```python\n",
    "def search(query):\n",
    "    boost = {'question': 3.0, 'section': 0.5}\n",
    "\n",
    "    results = index.search(\n",
    "        query=query,\n",
    "        filter_dict={'course': 'data-engineering-zoomcamp'},\n",
    "        boost_dict=boost,\n",
    "        num_results=5,\n",
    "        output_ids=True\n",
    "    )\n",
    "\n",
    "    return results\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can be jsonified for OpenAI like :\n",
    "\n",
    "```json\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"name\": \"search\",\n",
    "    \"description\": \"Search the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"query\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"query\"],\n",
    "        \"additionalProperties\": False\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "Slightly different syntax for OpenRouter (older OpenAI syntax ?):\n",
    "\n",
    "http://openrouter.ai/docs/features/tool-calling\n",
    "\n",
    "```json\n",
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\":{\n",
    "        \"name\": \"search\",\n",
    "        \"description\": \"Search the FAQ database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_tool = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\":{\n",
    "        \"name\": \"search\",\n",
    "        \"description\": \"Search the FAQ database\",\n",
    "        \"parameters\": {\n",
    "            \"type\": \"object\",\n",
    "            \"properties\": {\n",
    "                \"query\": {\n",
    "                    \"type\": \"string\",\n",
    "                    \"description\": \"Search query text to look up in the course FAQ.\"\n",
    "                }\n",
    "            },\n",
    "            \"required\": [\"query\"],\n",
    "            \"additionalProperties\": False\n",
    "        }\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to do well in Module 1 ?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant.\n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=chat_messages,\n",
    "    tools=tools\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ChatCompletionMessageToolCall(id='tool_0_search', function=Function(arguments='{\"query\":\"how to do well in Module 1\"}', name='search'), type='function', index=0)"
      ]
     },
     "execution_count": 292,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call = response.choices[0].message.tool_calls[0]\n",
    "call"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'tool_0_search'"
      ]
     },
     "execution_count": 293,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "call_id = call.id\n",
    "call_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search\n",
      "{'query': 'how to do well in Module 1'}\n"
     ]
    }
   ],
   "source": [
    "f_name = call.function.name\n",
    "arguments  = json.loads(\n",
    "    call.function.arguments\n",
    ")\n",
    "print(f_name)\n",
    "print(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get functions by name from globals()\n",
    "f = globals()[f_name]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = f(**arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\n",
      "  {\n",
      "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course inst\n"
     ]
    }
   ],
   "source": [
    "search_results = json.dumps(results, indent=2)\n",
    "print(search_results[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_messages.append(call)\n",
    "\n",
    "chat_messages.append({\n",
    "    \"role\": \"tool\",\n",
    "    \"tool_call_id\": call_id,\n",
    "    \"name\":f_name,\n",
    "    \"content\": json.dumps(\n",
    "        search_results\n",
    "    )\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'role': 'developer',\n",
       "  'content': \"You're a course teaching assistant.\\nYou're given a question from a course student and your task is to answer it.\"},\n",
       " {'role': 'user', 'content': 'How to do well in Module 1 ?'},\n",
       " ChatCompletionMessageToolCall(id='tool_0_search', function=Function(arguments='{\"query\":\"How to do well in Module 1\"}', name='search'), type='function', index=0),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'tool_0_search',\n",
       "  'name': 'search',\n",
       "  'content': '\"[\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\\\\\n!pip install findspark\\\\\\\\nimport findspark\\\\\\\\nfindspark.init()\\\\\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\\\\\nFilter based on conditions based on multiple columns\\\\\\\\nfrom pyspark.sql.functions import col\\\\\\\\nnew_final.filter((new_final.a_zone==\\\\\\\\\\\\\"Murray Hill\\\\\\\\\\\\\") & (new_final.b_zone==\\\\\\\\\\\\\"Midwood\\\\\\\\\\\\\")).show()\\\\\\\\nKrishna Anand\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 5: pyspark\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Module Not Found Error in Jupyter Notebook .\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 322\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\\\\\nexport PYTHONPATH=\\\\\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\\\\\u201d\\\\\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\\\\\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\\\\\\\\\"` appropriately.\\\\\\\\nAdditionally, you can check for the version of \\\\\\\\u2018py4j\\\\\\\\u2019 of the spark you\\\\\\\\u2019re using from here and update as mentioned above.\\\\\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 5: pyspark\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 323\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\\\\\nSolution:\\\\\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 4: analytics engineering with dbt\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 299\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\\\\\\\\\"TypeError: \\'module\\' object is not callable\\\\\\\\\\\\\"\\\\\\\\nSolution:\\\\\\\\nconn_string = \\\\\\\\\\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\\\\\\\\\"\\\\\\\\nengine = create_engine(conn_string)\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 1: Docker and Terraform\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Python - SQLALchemy - TypeError \\'module\\' object is not callable\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 124\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"Error raised during the jupyter notebook\\\\\\\\u2019s cell execution:\\\\\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\\\\\nSolution: Need to install Python module \\\\\\\\u201cpsycopg2\\\\\\\\u201d. Can be installed by Conda or pip.\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 1: Docker and Terraform\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 125\\\\n  }\\\\n]\"'},\n",
       " ChatCompletionMessageToolCall(id='tool_0_search', function=Function(arguments='{\"query\":\"how to do well in Module 1\"}', name='search'), type='function', index=0),\n",
       " {'role': 'tool',\n",
       "  'tool_call_id': 'tool_0_search',\n",
       "  'name': 'search',\n",
       "  'content': '\"[\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\\\\\\\nThe solution which worked for me(use following in jupyter notebook) :\\\\\\\\n!pip install findspark\\\\\\\\nimport findspark\\\\\\\\nfindspark.init()\\\\\\\\nThereafter , import pyspark and create spark contex<<t as usual\\\\\\\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\\\\\\\nFilter based on conditions based on multiple columns\\\\\\\\nfrom pyspark.sql.functions import col\\\\\\\\nnew_final.filter((new_final.a_zone==\\\\\\\\\\\\\"Murray Hill\\\\\\\\\\\\\") & (new_final.b_zone==\\\\\\\\\\\\\"Midwood\\\\\\\\\\\\\")).show()\\\\\\\\nKrishna Anand\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 5: pyspark\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Module Not Found Error in Jupyter Notebook .\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 322\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\\\\\\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\\\\\\\nexport PYTHONPATH=\\\\\\\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\\\\\\\u201d\\\\\\\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`.\\\\\\\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\\\\\\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\\\\\\\\\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\\\\\\\\\\\"` appropriately.\\\\\\\\nAdditionally, you can check for the version of \\\\\\\\u2018py4j\\\\\\\\u2019 of the spark you\\\\\\\\u2019re using from here and update as mentioned above.\\\\\\\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 5: pyspark\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Py4JJavaError - ModuleNotFoundError: No module named \\'py4j\\'` while executing `import pyspark`\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 323\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named \\'pytz\\'`\\\\\\\\nSolution:\\\\\\\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 4: analytics engineering with dbt\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"DBT - Error: No module named \\'pytz\\' while setting up dbt with docker\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 299\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\')  I get the error \\\\\\\\\\\\\"TypeError: \\'module\\' object is not callable\\\\\\\\\\\\\"\\\\\\\\nSolution:\\\\\\\\nconn_string = \\\\\\\\\\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\\\\\\\\\"\\\\\\\\nengine = create_engine(conn_string)\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 1: Docker and Terraform\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Python - SQLALchemy - TypeError \\'module\\' object is not callable\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 124\\\\n  },\\\\n  {\\\\n    \\\\\"text\\\\\": \\\\\"Error raised during the jupyter notebook\\\\\\\\u2019s cell execution:\\\\\\\\nengine = create_engine(\\'postgresql://root:root@localhost:5432/ny_taxi\\').\\\\\\\\nSolution: Need to install Python module \\\\\\\\u201cpsycopg2\\\\\\\\u201d. Can be installed by Conda or pip.\\\\\",\\\\n    \\\\\"section\\\\\": \\\\\"Module 1: Docker and Terraform\\\\\",\\\\n    \\\\\"question\\\\\": \\\\\"Python - SQLAlchemy - ModuleNotFoundError: No module named \\'psycopg2\\'.\\\\\",\\\\n    \\\\\"course\\\\\": \\\\\"data-engineering-zoomcamp\\\\\",\\\\n    \\\\\"_id\\\\\": 125\\\\n  }\\\\n]\"'}]"
      ]
     },
     "execution_count": 299,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I can't provide specific advice on how to \"do well\" in Module 1 as the search results primarily focus on troubleshooting technical errors related to different modules (Module 5: PySpark, Module 4: dbt, and Module 1: Docker and Terraform). \n",
       "\n",
       "However, looking at the Module 1 entries, these are common issues:\n",
       "\n",
       "*   **`TypeError: 'module' object is not callable` with SQLAlchemy:** This usually means you're trying to call a module as if it were a function. The solution provided suggests changing the connection string to `postgresql+psycopg://` and then correctly creating the engine.\n",
       "*   **`ModuleNotFoundError: No module named 'psycopg2'`**: This error indicates that the `psycopg2` Python module, necessary for interacting with PostgreSQL, is not installed. The solution is to install it using `conda` or `pip`.\n",
       "\n",
       "To do well in Module 1, I would recommend:\n",
       "\n",
       "1.  **Carefully review the module materials:** Understand the core concepts of Docker and Terraform.\n",
       "2.  **Pay close attention to setup instructions:** Many errors stem from incorrect environments or missing dependencies.\n",
       "3.  **Troubleshoot methodically:** When you encounter an error, try to understand the error message.\n",
       "4.  **Look for similar issues:** If you encounter a problem, it's likely others have too. Search the course FAQs, forums, or general programming resources.\n",
       "\n",
       "If you have a more specific question about a concept or a problem you're facing in Module 1, feel free to ask!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 303,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\n",
    "    response.choices[0].message.content\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making multiple calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "question = \"How to do well in Module 1 ?\"\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "If you look up something in FAQ, convert the student question into multiple queries.\n",
    "\"\"\".strip()\n",
    "\n",
    "tools = [search_tool]\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "    {\"role\": \"user\", \"content\": question}\n",
    "]\n",
    "# response will contain multiple tool calls\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_call(tool_call_response):\n",
    "    call_id = tool_call_response.id\n",
    "    function_name = tool_call_response.function.name\n",
    "    arguments = json.loads(tool_call_response.function.arguments)\n",
    "\n",
    "    f = globals()[function_name]\n",
    "    result = f(**arguments)\n",
    "\n",
    "    return {\n",
    "        \"role\": \"tool\",\n",
    "        \"tool_call_id\": call_id,\n",
    "        \"name\":function_name,\n",
    "        \"content\": json.dumps(\n",
    "            result, indent = 2\n",
    "            )\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [],
   "source": [
    "calls = response.choices[0].message.tool_calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 320,
   "metadata": {},
   "outputs": [],
   "source": [
    "for call in calls:\n",
    "    chat_messages.append(call)\n",
    "    \n",
    "    result = do_call(call)\n",
    "    chat_messages.append(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# response will contain multiple tool calls\n",
    "response = client.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=chat_messages,\n",
    "    tools=tools\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "I can't provide specific advice on how to \"do well\" in Module 1, as that depends on the course content and your learning style. However, I can help you with specific technical issues you might encounter in Module 1.\n",
       "\n",
       "Are you running into any particular errors or having trouble with any of the concepts? For example, some common issues in Module 1 of the Data Engineering Zoomcamp course include:\n",
       "\n",
       "*   **`ModuleNotFoundError: No module named 'psycopg2'`**: This error occurs when the `psycopg2` Python module, needed for PostgreSQL, is not installed. You can usually fix this by running `pip install psycopg2-binary`.\n",
       "*   **`TypeError: 'module' object is not callable` with SQLAlchemy**: This error often means you're trying to call `create_engine` incorrectly. Make sure you are using `create_engine` with the correct connection string format, e.g., `postgresql+psycopg://user:password@host:port/database_name`.\n",
       "\n",
       "If you have other questions related to specific errors or topics in Module 1, please let me know!"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 323,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Markdown(\n",
    "    response.choices[0].message.content\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tool_calls\n"
     ]
    }
   ],
   "source": [
    "print(response.choices[0].finish_reason)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ if your own knowledge is not sufficient to answer the question.\n",
    "When using FAQ, perform deep topic exploration: make one request to FAQ,\n",
    "and then based on the results, make more requests.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_messages = [\n",
    "    {\"role\": \"developer\", \"content\": developer_prompt},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Module 1 of the Data Engineering Zoomcamp covers Docker and Terraform. This includes topics like ingesting NY Taxi Data to Postgres, and resolving `psycopg2` and SQLAlchemy-related errors.\n",
       "\n",
       "Module 4 focuses on Analytics Engineering with dbt. This module addresses issues like connecting dbt Cloud with BigQuery, resolving `NoneType` object errors in dbt macros, and handling `ModuleNotFoundError: No module named 'pytz'` when setting up dbt with Docker. It also covers what to do if `dbt run` doesn't update your BigQuery table as expected.\n",
       "\n",
       "What else would you like to know about these modules or any other part of the course?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "Module 4 of the Data Engineering Zoomcamp covers \"Analytics Engineering with dbt\". Here are some key takeaways:\n",
       "\n",
       "*   **dbt Project Setup:** Ensure your `dbt_project.yml` file is correctly configured, especially if your dbt project is in a subdirectory.\n",
       "*   **Connecting dbt Cloud with BigQuery:** You might encounter \"Access Denied\" errors. To resolve this, grant your dbt service account the \"BigQuery Job User\" role in Google Cloud's IAM & Admin. It's also recommended to add \"BigQuery Data Owner,\" \"Storage Object Admin,\" and \"Storage Admin\" roles to prevent future permission issues.\n",
       "*   **Handling Data Types:** Be mindful of column data types when working with dbt and BigQuery, especially when converting from CSV to Parquet. If dbt infers incorrect types, define the schema explicitly in your data ingestion pipeline or cast the columns within your dbt models. Pandas `Int64` (with a capital 'I') can be used for nullable integers when reading CSVs.\n",
       "*   **Variable Usage:** When using variables with `dbt run`, ensure proper YAML dictionary format, e.g., `dbt run --var 'is_test_run: false'`.\n",
       "*   **CI/CD Jobs in dbt Cloud:** If \"Triggered by pull requests\" is disabled when setting up a Continuous Integration job, it's likely due to being on the Developer Plan. You'll need to upgrade to a Team Plan or Enterprise Plan to utilize CI Jobs.\n",
       "\n",
       "Do you have any specific questions about any of these dbt topics, or would you like to explore another module?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/markdown": [
       "`dbt run` compiles and executes all models in your dbt project, creating or updating the corresponding tables or views in your data warehouse. `dbt build` is a superset of `dbt run`; it not only runs your models but also includes `dbt test`, `dbt snapshot`, and `dbt seed` operations. This means `dbt build` will execute your tests, apply any data snapshots, and load any seed files in addition to building your models.\n",
       "\n",
       "Here's why you might choose one over the other:\n",
       "\n",
       "*   **`dbt run`**: Ideal for everyday development when you're primarily focused on building and transforming your data models. It's faster as it skips tests and other operations.\n",
       "*   **`dbt build`**: Recommended for a more comprehensive workflow, especially before deploying to production or when you want to ensure data quality and integrity. It provides a more robust check of your dbt project.\n",
       "\n",
       "Do you have any specific issues you're encountering with `dbt run` or `dbt build`?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "while True: # main Q&A loop\n",
    "    question = input() # How do I do my best for module 1?\n",
    "    if question == 'stop':\n",
    "        break\n",
    "\n",
    "    message = {\"role\": \"user\", \"content\": question}\n",
    "    chat_messages.append(message)\n",
    "\n",
    "    while True: # request-response loop - query API till get a message\n",
    "        # response will contain multiple tool calls\n",
    "        response = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=chat_messages,\n",
    "            tools=tools\n",
    "        )\n",
    "\n",
    "        # check for tool calls\n",
    "        has_tool_calls = response.choices[0].finish_reason == 'tool_calls'\n",
    "        \n",
    "        if has_tool_calls:\n",
    "            \n",
    "            # consume all tool calls\n",
    "            calls = response.choices[0].message.tool_calls\n",
    "            \n",
    "            for call in calls:\n",
    "                chat_messages.append(call)\n",
    "                \n",
    "                result = do_call(call)\n",
    "                chat_messages.append(result)\n",
    "            \n",
    "        if response.choices[0].finish_reason == 'stop':\n",
    "            display(\n",
    "                Markdown(\n",
    "                    response.choices[0].message.content\n",
    "                )\n",
    "            )\n",
    "            break\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_entry(question, answer):\n",
    "    doc = {\n",
    "        'question': question,\n",
    "        'text': answer,\n",
    "        'section': 'user added',\n",
    "        'course': 'data-engineering-zoomcamp'\n",
    "    }\n",
    "    index.append(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "add_entry_description = {\n",
    "    \"type\": \"function\",\n",
    "    \"function\": {\"name\": \"add_entry\",\n",
    "    \"description\": \"Add an entry to the FAQ database\",\n",
    "    \"parameters\": {\n",
    "        \"type\": \"object\",\n",
    "        \"properties\": {\n",
    "            \"question\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The question to be added to the FAQ database\",\n",
    "            },\n",
    "            \"answer\": {\n",
    "                \"type\": \"string\",\n",
    "                \"description\": \"The answer to the question\",\n",
    "            }\n",
    "        },\n",
    "        \"required\": [\"question\", \"answer\"],\n",
    "        \"additionalProperties\": False\n",
    "    }}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'type': 'function', 'function': {'name': 'search', 'description': 'Search the FAQ database', 'parameters': {'type': 'object', 'properties': {'query': {'type': 'string', 'description': 'Search query text to look up in the course FAQ.'}}, 'required': ['query'], 'additionalProperties': False}}}, {'type': 'function', 'function': {'name': 'add_entry', 'description': 'Add an entry to the FAQ database', 'parameters': {'type': 'object', 'properties': {'question': {'type': 'string', 'description': 'The question to be added to the FAQ database'}, 'answer': {'type': 'string', 'description': 'The answer to the question'}}, 'required': ['question', 'answer'], 'additionalProperties': False}}}]\n"
     ]
    }
   ],
   "source": [
    "import chat_assistant\n",
    "from importlib import reload\n",
    "chat_assistant = reload(chat_assistant)\n",
    "\n",
    "tools = chat_assistant.Tools()\n",
    "tools.add_tool(search, search_tool)\n",
    "tools.add_tool(add_entry, add_entry_description)\n",
    "print(tools.get_tools())\n",
    "\n",
    "developer_prompt = \"\"\"\n",
    "You're a course teaching assistant. \n",
    "You're given a question from a course student and your task is to answer it.\n",
    "\n",
    "Use FAQ to augment your answer.\n",
    "\n",
    "At the end of each response, ask the user a follow up question based on your answer.\n",
    "\n",
    "Use the add_entry tool to add entries to the FAQ database when asked to do so.\n",
    "\"\"\".strip()\n",
    "\n",
    "chat_interface = chat_assistant.ChatInterface()\n",
    "\n",
    "chat = chat_assistant.ChatAssistant(\n",
    "    tools=tools,\n",
    "    developer_prompt=developer_prompt,\n",
    "    chat_interface=chat_interface,\n",
    "    client=client,\n",
    "    model=MODEL\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>search({\"query\":\"module 1\"})</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ChatCompletionMessageToolCall(id='tool_0_search', function=Function(arguments='{\"query\":\"module 1\"}', name='search'), type='function', index=0)</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>[\n",
       "  {\n",
       "    \"text\": \"You need to look for the Py4J file and note the version of the filename. Once you know the version, you can update the export command accordingly, this is how you check yours:\\n` ls ${SPARK_HOME}/python/lib/ ` and then you add it in the export command, mine was:\\nexport PYTHONPATH=\\u201d${SPARK_HOME}/python/lib/Py4J-0.10.9.5-src.zip:${PYTHONPATH}\\u201d\\nMake sure that the version under `${SPARK_HOME}/python/lib/` matches the filename of py4j or you will encounter `ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`.\\nFor instance, if the file under `${SPARK_HOME}/python/lib/` was `py4j-0.10.9.3-src.zip`.\\nThen the export PYTHONPATH statement above should be changed to `export PYTHONPATH=\\\"${SPARK_HOME}/python/lib/py4j-0.10.9.3-src.zip:$PYTHONPATH\\\"` appropriately.\\nAdditionally, you can check for the version of \\u2018py4j\\u2019 of the spark you\\u2019re using from here and update as mentioned above.\\n~ Abhijit Chakraborty: Sometimes, even with adding the correct version of py4j might not solve the problem. Simply run pip install py4j and problem should be resolved.\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Py4JJavaError - ModuleNotFoundError: No module named 'py4j'` while executing `import pyspark`\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 323\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Following dbt with BigQuery on Docker readme.md, after `docker-compose build` and `docker-compose run dbt-bq-dtc init`, encountered error `ModuleNotFoundError: No module named 'pytz'`\\nSolution:\\nAdd `RUN python -m pip install --no-cache pytz` in the Dockerfile under `FROM --platform=$build_for python:3.9.9-slim-bullseye as base`\",\n",
       "    \"section\": \"Module 4: analytics engineering with dbt\",\n",
       "    \"question\": \"DBT - Error: No module named 'pytz' while setting up dbt with docker\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 299\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Even after installing pyspark correctly on linux machine (VM ) as per course instructions, faced a module not found error in jupyter notebook .\\nThe solution which worked for me(use following in jupyter notebook) :\\n!pip install findspark\\nimport findspark\\nfindspark.init()\\nThereafter , import pyspark and create spark contex<<t as usual\\nNone of the solutions above worked for me till I ran !pip3 install pyspark instead !pip install pyspark.\\nFilter based on conditions based on multiple columns\\nfrom pyspark.sql.functions import col\\nnew_final.filter((new_final.a_zone==\\\"Murray Hill\\\") & (new_final.b_zone==\\\"Midwood\\\")).show()\\nKrishna Anand\",\n",
       "    \"section\": \"Module 5: pyspark\",\n",
       "    \"question\": \"Module Not Found Error in Jupyter Notebook .\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 322\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"Issue:\\ne\\u2026\\nSolution:\\npip install psycopg2-binary\\nIf you already have it, you might need to update it:\\npip install psycopg2-binary --upgrade\\nOther methods, if the above fails:\\nif you are getting the \\u201c ModuleNotFoundError: No module named 'psycopg2' \\u201c error even after the above installation, then try updating conda using the command conda update -n base -c defaults conda. Or if you are using pip, then try updating it before installing the psycopg packages i.e\\nFirst uninstall the psycopg package\\nThen update conda or pip\\nThen install psycopg again using pip.\\nif you are still facing error with r pcycopg2 and showing pg_config not found then you will have to install postgresql. in MAC it is brew install postgresql\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Postgres - ModuleNotFoundError: No module named 'psycopg2'\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 112\n",
       "  },\n",
       "  {\n",
       "    \"text\": \"create_engine('postgresql://root:root@localhost:5432/ny_taxi')  I get the error \\\"TypeError: 'module' object is not callable\\\"\\nSolution:\\nconn_string = \\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\"\\nengine = create_engine(conn_string)\",\n",
       "    \"section\": \"Module 1: Docker and Terraform\",\n",
       "    \"question\": \"Python - SQLALchemy - TypeError 'module' object is not callable\",\n",
       "    \"course\": \"data-engineering-zoomcamp\",\n",
       "    \"_id\": 124\n",
       "  }\n",
       "]</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>Module 1 of the course covers Docker and Terraform. It includes setting up your environment and using these tools for data engineering tasks. Some common issues students face in this module include <code>ModuleNotFoundError: No module named 'psycopg2'</code> and <code>TypeError: 'module' object is not callable</code> when working with SQLAlchemy and PostgreSQL.</p>\n",
       "<p>To resolve the <code>psycopg2</code> error, you can try <code>pip install psycopg2-binary</code> or upgrade your existing installation. If you're still facing issues, ensure your <code>conda</code> or <code>pip</code> is updated, then uninstall and reinstall the package. For the SQLAlchemy <code>TypeError</code>, ensure you're using <code>create_engine</code> correctly with the connection string, for example: <code>engine = create_engine(\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\")</code>.</p>\n",
       "<p>What specific topics in Docker or Terraform are you interested in learning more about?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <details>\n",
       "            <summary>Function call: <tt>add_entry({\"question\":\"What is covered in Module 1?\",\"ans...)</tt></summary>\n",
       "            <div>\n",
       "                <b>Call</b>\n",
       "                <pre>ChatCompletionMessageToolCall(id='tool_0_add_entry', function=Function(arguments='{\"question\":\"What is covered in Module 1?\",\"answer\":\"Module 1 of the course covers Docker and Terraform. It includes setting up your environment and using these tools for data engineering tasks. Common issues and solutions for this module are:\\\\n- `ModuleNotFoundError: No module named \\'psycopg2\\'`: Try `pip install psycopg2-binary` or upgrade your existing installation. If issues persist, update `conda` or `pip`, then uninstall and reinstall the package.\\\\n- `TypeError: \\'module\\' object is not callable` with SQLAlchemy: Ensure correct `create_engine` usage, e.g., `engine = create_engine(\\\\\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\\\\\")`.\"}', name='add_entry'), type='function', index=0)</pre>\n",
       "            </div>\n",
       "            <div>\n",
       "                <b>Output</b>\n",
       "                <pre>null</pre>\n",
       "            </div>\n",
       "            \n",
       "            </details>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <div><b>Assistant:</b></div>\n",
       "                <div><p>I have added the information about Module 1, Docker, and Terraform, along with common issues and their solutions, to the FAQ database.</p>\n",
       "<p>What other modules or topics are you curious about?</p></div>\n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chat ended.\n"
     ]
    }
   ],
   "source": [
    "chat.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://ai.pydantic.dev/models/openai/#openrouter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 434,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic_ai import Agent, RunContext\n",
    "from pydantic_ai.providers.openrouter import OpenRouterProvider\n",
    "from pydantic_ai.models.openai import OpenAIModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 435,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = OpenAIModel(\n",
    "    MODEL,  \n",
    "    provider = OpenRouterProvider(api_key=OPENROUTER_API_KEY),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 436,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_agent = Agent(\n",
    "    model,\n",
    "    system_prompt=developer_prompt\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Dict\n",
    "\n",
    "\n",
    "@chat_agent.tool\n",
    "def search_tool(ctx: RunContext, query: str) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Search the FAQ for relevant entries matching the query.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    query : str\n",
    "        The search query string provided by the user.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    list\n",
    "        A list of search results (up to 5), each containing relevance information \n",
    "        and associated output IDs.\n",
    "    \"\"\"\n",
    "    print(f\"search('{query}')\")\n",
    "    return search(query)\n",
    "\n",
    "\n",
    "@chat_agent.tool\n",
    "def add_entry_tool(ctx: RunContext, question: str, answer: str) -> None:\n",
    "    \"\"\"\n",
    "    Add a new question-answer entry to FAQ.\n",
    "\n",
    "    This function creates a document with the given question and answer, \n",
    "    tagging it as user-added content.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    question : str\n",
    "        The question text to be added to the index.\n",
    "\n",
    "    answer : str\n",
    "        The answer or explanation corresponding to the question.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    None\n",
    "    \"\"\"\n",
    "    return add_entry(question, answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search('tools used in the course')\n",
      "search('common issues with tools')\n"
     ]
    },
    {
     "data": {
      "text/markdown": [
       "The course covers two alternative data stacks: one using Google Cloud Platform (GCP) and another using local installations of various tools. You have the flexibility to choose one of these or even use your own preferred tools, although support will be limited if you deviate from the course's recommended stacks.\n",
       "\n",
       "Here are some of the tools mentioned:\n",
       "\n",
       "*   **Orchestration:** Airflow, Prefect, or Mage\n",
       "*   **Cloud Providers:** AWS or GCP products\n",
       "*   **Business Intelligence:** Tableau, Metabase, or Google Data Studio\n",
       "*   **Version Control:** Git/GitHub\n",
       "*   **Containerization:** Docker\n",
       "*   **Infrastructure as Code:** Terraform\n",
       "*   **Database related:** Pandas, SQLAlchemy, psycopg2\n",
       "\n",
       "Common issues that students encounter with these tools, and general troubleshooting tips, include:\n",
       "\n",
       "*   **`ModuleNotFoundError: No module named 'psycopg2'`**: This often happens when `psycopg2` is not installed correctly. Try `pip install psycopg2-binary` or upgrade your existing installation. If problems persist, update `conda` or `pip`, then try uninstalling and reinstalling the package.\n",
       "*   **`TypeError: 'module' object is not callable` with SQLAlchemy**: Ensure you are using `create_engine` correctly, for example: `engine = create_engine(\"postgresql+psycopg://root:root@localhost:5432/ny_taxi\")`.\n",
       "*   **Pandas missing value issues**: When injecting data into tools like BigQuery, you might face type errors because pandas can parse integer columns with missing values as float types by default. To address this, you can specify or cast the data type to `Int64` during data transformation. Alternatively, you can use `df.convert_dtypes()` after filling missing values (e.g., `df.fillna(-999999, inplace=True)`).\n",
       "*   **Google Cloud SDK PATH issues on Windows**: If you're consistently getting errors about the installer being unable to automatically update your system PATH, you might need to add Git Bash to your Windows path. This can be done by installing Anaconda Navigator and ensuring you check \"add conda to the path\" during installation, and/or installing Git Bash and checking \"Add GitBash to Windows Terminal\" and \"Use Git and optional Unix tools from the command prompt.\" Then, from within Git Bash, run `conda init bash`.\n",
       "*   **Troubleshooting in general**:\n",
       "    *   Start by trying to solve the issue yourself by reading error messages and documentation.\n",
       "    *   Restart the application, server, or PC.\n",
       "    *   Search for the error online using specific keywords (e.g., `<technology> <problem statement>`).\n",
       "    *   Check the tool's official documentation.\n",
       "    *   Consider uninstalling and reinstalling the application.\n",
       "    *   If all else fails, ask for help in the Slack channel, providing as much detail as possible (OS, commands run, exact error messages, what you've already tried). **Always copy and paste error messages, do not use screenshots.**\n",
       "\n",
       "What specific tools are you most interested in learning more about, or what kind of issues have you encountered so far?"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 442,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_prompt = \"what are the tools used in the course and what are common issues with these tools ? use as many search queries as needed\"\n",
    "agent_run = await chat_agent.run(user_prompt)\n",
    "Markdown(agent_run.output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "zcllm",
   "language": "python",
   "name": "zcllm"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
